{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe32251",
   "metadata": {},
   "source": [
    "# Example: cylinder as a large-scale problem\n",
    "\n",
    "This notebook shows how to solve a large-scale multi-parameter problem on a GPU machine. The problem is similar to the one in [cylinder_small_scale.ipynb](cylinder_small_scale.ipynb) but with a higher resolution for both the parameter distributions and the $q$-vectors. **Note that you need a GPU and 400 GB RAM to run this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ba28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import torch\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from ffsas.models import Cylinder\n",
    "from ffsas.system import SASGreensSystem\n",
    "from ffsas.utils import _form_batch_ids\n",
    "\n",
    "# avoid an OMP error on MacOS (nothing to do with ffsas)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7137f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dir\n",
    "output_dir = Path('./output_large_scale')\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# reproduce figures in the paper \n",
    "reproduce_paper_fig = True\n",
    "if reproduce_paper_fig:\n",
    "    # this will trigger an error if latex is not installed\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": True,\n",
    "        \"text.latex.preamble\": r'\\usepackage{bm,upgreek}',\n",
    "        \"font.family\": \"sans-serif\",\n",
    "        \"font.serif\": [\"Times\"]})\n",
    "    # figure dir\n",
    "    paper_fig_dir = Path('../paper_figs')\n",
    "    Path(paper_fig_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b1f1f",
   "metadata": {},
   "source": [
    "Use the following flag to read existing results from `output_dir` without computation. This is for quick examination and visualization of the existing results. **Turn it to ``False`` if you are running this notebook for the first time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_results_from_file = True\n",
    "\n",
    "if read_results_from_file:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    # you need a GPU to run the inversion\n",
    "    assert torch.cuda.is_available()\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2993f4c",
   "metadata": {},
   "source": [
    "# Ground truth\n",
    "\n",
    "\n",
    "### Ground truth of distributions of $l$, $r$, $\\theta$, $\\phi$\n",
    "\n",
    "\n",
    "The following function creates a \"crazy\" distribution by adding up a few Gaussians and random noises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49329fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_distribution(x, gaussians, noise_level, fade_start, fade_end, noise_interval=1, seed=0):\n",
    "    # create\n",
    "    w_true = torch.zeros(x.shape)\n",
    "    \n",
    "    # add Gaussians\n",
    "    for factor, mean, stddev in gaussians:\n",
    "        w_true += factor * torch.exp(-((x - mean) / stddev) ** 2)\n",
    "    \n",
    "    # add noise\n",
    "    torch.random.manual_seed(seed)\n",
    "    len_noise = int(round(len(x) / noise_interval))\n",
    "    xn0 = torch.arange(1, len_noise + 1) * noise_interval\n",
    "    xn1 = torch.arange(1, len(x) + 1)\n",
    "    noise1 = torch.from_numpy(interp1d(xn0, torch.rand(len_noise), fill_value=\"extrapolate\")(xn1))\n",
    "    noise2 = torch.from_numpy(interp1d(xn0, torch.rand(len_noise), fill_value=\"extrapolate\")(xn1))\n",
    "    w_true += noise_level * noise1 * noise2\n",
    "    \n",
    "    # fade both ends to make it look nicer\n",
    "    if len(x) >= 3:\n",
    "        w_true[0:fade_start] = 0.\n",
    "        w_true[fade_start:fade_end] *= torch.linspace(0, 1, fade_end - fade_start)\n",
    "        w_true[-fade_start:] = 0.\n",
    "        w_true[-fade_end:-fade_start] *= torch.linspace(1, 0, fade_end - fade_start)\n",
    "    \n",
    "    # normalize to 1\n",
    "    w_true /= torch.sum(w_true)\n",
    "    return w_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "par_dict = {\n",
    "    'l': torch.linspace(200., 600., 40),\n",
    "    'r': torch.linspace(50., 90., 40),\n",
    "    'theta': torch.linspace(20., 75., 40),\n",
    "    'phi': torch.linspace(150., 240., 40)\n",
    "}\n",
    "\n",
    "# parameter distributions\n",
    "w_true_dict = {\n",
    "    'l': crazy_distribution(par_dict['l'], [(1.5, 300, 20), (1, 400, 20), (2, 500, 20)], 1, 1, 1, \n",
    "                            noise_interval=1.2),\n",
    "    'r': crazy_distribution(par_dict['r'], [(1, 60, 3), (2, 70, 4), (2, 80, 3)], 1, 1, 1, \n",
    "                            noise_interval=1.2),\n",
    "    'theta': crazy_distribution(par_dict['theta'], [(4, 30, 5), (2, 50, 5), (2, 65, 5)], 2, 1, 1, \n",
    "                                noise_interval=1.2),\n",
    "    'phi': crazy_distribution(par_dict['phi'], [(2, 170, 10), (2, 200, 10), (4, 220, 10)], 3, 1, 1, \n",
    "                              noise_interval=1.2)\n",
    "}\n",
    "\n",
    "# plot distributions\n",
    "fig, ax = plt.subplots(2, 2, dpi=200, figsize=(10, 6))\n",
    "plt.subplots_adjust(hspace=.5, wspace=.3)\n",
    "ax[0, 0].plot(par_dict['l'], w_true_dict['l'], label='Truth')\n",
    "ax[0, 1].plot(par_dict['r'], w_true_dict['r'], label='Truth')\n",
    "ax[1, 0].plot(par_dict['theta'], w_true_dict['theta'], label='Truth')\n",
    "ax[1, 1].plot(par_dict['phi'], w_true_dict['phi'], label='Truth')\n",
    "ax[0, 0].set_xlabel(r'Lenght, $l$ (\\AA)')\n",
    "ax[0, 1].set_xlabel(r'Radius, $r$ (\\AA)')\n",
    "ax[1, 0].set_xlabel(r'Cylinder axis to beam angle, $\\theta$ (degree)')\n",
    "ax[1, 1].set_xlabel(r'Rotation about beam, $\\phi$ (degree)')\n",
    "ax[0, 0].set_ylabel(r'Weights, $w$')\n",
    "ax[0, 1].set_ylabel(r'Weights, $w$')\n",
    "ax[1, 0].set_ylabel(r'Weights, $w$')\n",
    "ax[1, 1].set_ylabel(r'Weights, $w$')\n",
    "plt.show()\n",
    "\n",
    "# degree to radian\n",
    "par_dict['theta'] = torch.deg2rad(par_dict['theta'])\n",
    "par_dict['phi'] = torch.deg2rad(par_dict['phi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1c15b",
   "metadata": {},
   "source": [
    "### Ground truth of intensity\n",
    "\n",
    "Now we compute the true intensity from the above parameter distributions, assuming the truth of $\\xi$ and $b$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ground truth of scale and background\n",
    "scale_true = 0.15\n",
    "b_true = 2.2e-4\n",
    "\n",
    "# q vectors\n",
    "# log scale on the sides\n",
    "q_side = torch.logspace(-2, 0, 50)\n",
    "# linear scale in the center\n",
    "q_center = torch.linspace(-0.0095, 0.0095, 20)\n",
    "qx = torch.cat((-torch.from_numpy(q_side.numpy()[::-1].copy()), q_center, q_side))\n",
    "qy = qx.clone()\n",
    "\n",
    "# (SLD - SLD_solvent) ^ 2\n",
    "drho = 1.\n",
    "\n",
    "# batch size\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e31fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not read_results_from_file:\n",
    "    # compute the Green's tensor\n",
    "    G = Cylinder.compute_G_mini_batch([qx, qy], par_dict, {'drho': drho}, batch_size=batch_size, device=device,\n",
    "                                      fixed_par_weights=None)\n",
    "\n",
    "    # define the G-based SAS system\n",
    "    g_sys = SASGreensSystem(G, ['l', 'r', 'theta', 'phi'], batch_size=batch_size, device=device)\n",
    "\n",
    "    # true xi\n",
    "    V = Cylinder.compute_V(par_dict)\n",
    "    V_ave = torch.tensordot(torch.tensordot(V, w_true_dict['r'], dims=1), w_true_dict['l'], dims=1)\n",
    "    xi_true = (1e-4 * scale_true / V_ave).item()\n",
    "\n",
    "    # finally compute the ground truth of intensity\n",
    "    I_true = g_sys.compute_intensity(w_true_dict, xi_true, b_true)\n",
    "\n",
    "    # free large memory of G\n",
    "    del G, g_sys\n",
    "\n",
    "    # save\n",
    "    with open(output_dir / 'I_true.pkl', 'wb') as f:\n",
    "        pickle.dump(I_true, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55335c5",
   "metadata": {},
   "source": [
    "Plot the intensity truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file\n",
    "with open(output_dir / 'I_true.pkl', 'rb') as f:\n",
    "    I_true = pickle.load(f)\n",
    "    \n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(I_true.t(), extent=(qx[0], qx[-1], qy[0], qy[-1]), aspect=1., cmap='turbo',\n",
    "           norm=colors.LogNorm(vmin=I_true.min(), vmax=I_true.max()))\n",
    "plt.xlabel(r'Scattering vector, $qx$ ($\\AA^{-1}$)')\n",
    "plt.ylabel(r'Scattering vector, $qy$ ($\\AA^{-1}$)')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a920e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#  Inversion\n",
    "\n",
    "We first conduct a low-resolution solution to provide a good initial guess for the high-resolution one. We lower the resolution of $q_x$ and $q_y$ from 120 to 40, keeping the resolution of the parameter distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80d8b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not read_results_from_file:\n",
    "    # q-vector decimated by 3\n",
    "    q_decimate = 3\n",
    "    G_low = Cylinder.compute_G_mini_batch([qx[q_decimate // 2::q_decimate], qy[q_decimate // 2::q_decimate]], \n",
    "                                          par_dict, {'drho': drho}, batch_size=batch_size, device=device,\n",
    "                                          fixed_par_weights=None)\n",
    "\n",
    "    # define the G-based SAS system\n",
    "    g_sys_low = SASGreensSystem(G_low, ['l', 'r', 'theta', 'phi'], batch_size=batch_size, device=device)\n",
    "\n",
    "    # solve low-resolution for 40 iterations\n",
    "    I_true_low = I_true[q_decimate // 2::q_decimate, q_decimate // 2::q_decimate].clone()\n",
    "    res_dict_low = g_sys_low.solve_inverse(I_true_low, I_true_low, auto_scaling=True,\n",
    "                                           maxiter=30, verbose=2, trust_options={'xtol': 0.},\n",
    "                                           returns_intensity_sensitivity_uncertainty=False)\n",
    "\n",
    "    # save\n",
    "    with open(output_dir / 'res_dict_low.pkl', 'wb') as f:\n",
    "        pickle.dump(res_dict_low, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1891c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file\n",
    "with open(output_dir / 'res_dict_low.pkl', 'rb') as f:\n",
    "    res_dict_low = pickle.load(f)\n",
    "    \n",
    "# quick plot\n",
    "fig, ax = plt.subplots(1, 4, dpi=100, figsize=(15, 2))\n",
    "for i, (key, val) in enumerate(res_dict_low['w_dict'].items()):\n",
    "    ax[i].plot(w_true_dict[key])\n",
    "    ax[i].plot(val)\n",
    "    ax[i].set_xlabel(key)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0abf2",
   "metadata": {},
   "source": [
    "Now we perform the high-resolution inversion. We monitor the convergence of each parameter every `check_iter` iterations. If a parameter distribution converges (L1 increment < `L1_tol`), we mark it as constant for further iterations. Only one dimension reduction will significantly speedup the solution. After one dimension reduction, we reduce `L1_tol` by 10 times (annealing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c8226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not read_results_from_file:\n",
    "    # use the low-resolution solution as the current state of variables\n",
    "    # to be updated during solution\n",
    "    w_dict_cur = res_dict_low['w_dict']\n",
    "    xi_cur = res_dict_low['xi']\n",
    "    b_cur = res_dict_low['b']\n",
    "\n",
    "    # compute the Green's tensor and system\n",
    "    # to be updated during solution if dimension reduction occurs\n",
    "    variables = ['l', 'r', 'theta', 'phi']\n",
    "    G = Cylinder.compute_G_mini_batch([qx, qy], par_dict, {'drho': drho}, \n",
    "                                      batch_size=batch_size, device=device,\n",
    "                                      fixed_par_weights=None)\n",
    "    g_sys = SASGreensSystem(G, variables, batch_size=batch_size, device=device)\n",
    "\n",
    "    # batch ids for reducing G\n",
    "    batch_ids = _form_batch_ids([len(qx), len(qy)], batch_size)\n",
    "\n",
    "    # L1 tolerance\n",
    "    L1_tol = 1e-3\n",
    "    L1_annealing = .1\n",
    "    # check interval\n",
    "    check_iter = 10\n",
    "\n",
    "    # result history\n",
    "    res_hist = []\n",
    "    reduce_hist = []\n",
    "    wtimes = []\n",
    "    time0 = time()\n",
    "\n",
    "    # super loop\n",
    "    for i_check in range(0, 200):\n",
    "        # invert the G-system\n",
    "        res_dict = g_sys.solve_inverse(I_true, I_true, auto_scaling=True, \n",
    "                                       maxiter=check_iter, verbose=2, trust_options={'xtol': 0.},\n",
    "                                       w_dict_init=w_dict_cur, xi_init=xi_cur, b_init=b_cur,\n",
    "                                       returns_intensity_sensitivity_uncertainty=False)\n",
    "\n",
    "        # quick plot\n",
    "        fig, ax = plt.subplots(1, 4, dpi=100, figsize=(15, 2))\n",
    "        for i, (key, val) in enumerate(res_dict['w_dict'].items()):\n",
    "            ax[i].plot(w_true_dict[key])\n",
    "            ax[i].plot(val)\n",
    "            ax[i].set_xlabel(key)\n",
    "        plt.show()\n",
    "\n",
    "        # save to history\n",
    "        res_hist.append(res_dict)\n",
    "        wtimes.append(time() - time0)\n",
    "        print(f'Done iter={(i_check + 1) * check_iter}, wt={wtimes[-1]}, variables={variables}')\n",
    "\n",
    "        # check convergence\n",
    "        L1_diff_min = 1e100\n",
    "        ivar_min = -1\n",
    "        for ivar, var in enumerate(variables):\n",
    "            L1_diff = torch.max(torch.abs(res_dict['w_dict'][var] - w_dict_cur[var]))\n",
    "            print(f'L1 diff of {var}: {L1_diff.item()}')\n",
    "            if L1_diff < L1_diff_min:\n",
    "                L1_diff_min = L1_diff\n",
    "                ivar_min = ivar\n",
    "\n",
    "        if L1_diff_min < L1_tol:\n",
    "            var = variables[ivar_min]\n",
    "            print(f'**************** Reducing dimension {var}, L1_diff={L1_diff_min} ****************')\n",
    "            reduce_hist.append([var, i_check])\n",
    "\n",
    "            # new variables\n",
    "            variables.remove(var)\n",
    "\n",
    "            # allocate reduced Green's function\n",
    "            shape = [len(qx), len(qy)]\n",
    "            for v in variables:\n",
    "                shape.append(len(par_dict[v]))\n",
    "            G_new = torch.zeros(shape)\n",
    "\n",
    "            # compute reduced Green's function by mini-batch\n",
    "            w_var = res_dict['w_dict'][var].to(device)\n",
    "            for i, batch_id in enumerate(batch_ids):\n",
    "                G_batch = G[tuple(batch_id)].to(device)\n",
    "                G_new_batch = torch.tensordot(G_batch, w_var, dims=[[ivar_min + 2], [0]])\n",
    "                G_new[tuple(batch_id)] = G_new_batch.to('cpu')\n",
    "            \n",
    "            # define the G-based SAS system\n",
    "            del G, g_sys\n",
    "            G = G_new\n",
    "            g_sys = SASGreensSystem(G, variables, batch_size=batch_size, device=device)\n",
    "\n",
    "            # annealing\n",
    "            L1_tol *= L1_annealing\n",
    "\n",
    "        # all converged\n",
    "        if len(variables) == 0:\n",
    "            print('All variables converged.')\n",
    "            break\n",
    "\n",
    "        # update current\n",
    "        w_dict_cur = res_dict['w_dict']\n",
    "        xi_cur = res_dict['xi']\n",
    "        b_cur = res_dict['b']\n",
    "\n",
    "    # save so we can skip this cell next time\n",
    "    with open(output_dir / 'res_hist.pkl', 'wb') as f:\n",
    "        pickle.dump(res_hist, f)\n",
    "\n",
    "    with open(output_dir / 'reduce_hist.pkl', 'wb') as f:\n",
    "        pickle.dump(reduce_hist, f)  \n",
    "\n",
    "    with open(output_dir / 'wtimes.pkl', 'wb') as f:\n",
    "        pickle.dump(wtimes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200913ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Processing\n",
    "\n",
    "First, read and process the convergence history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file\n",
    "with open(output_dir / 'res_hist.pkl', 'rb') as f:\n",
    "    res_hist = pickle.load(f)\n",
    "    \n",
    "with open(output_dir / 'reduce_hist.pkl', 'rb') as f:\n",
    "    reduce_hist = pickle.load(f)\n",
    "\n",
    "with open(output_dir / 'wtimes.pkl', 'rb') as f:\n",
    "    wtimes = pickle.load(f)\n",
    "    \n",
    "# final results\n",
    "variables = ['l', 'r', 'theta', 'phi']\n",
    "final_xi = res_hist[-1]['xi']\n",
    "final_b = res_hist[-1]['b']\n",
    "final_w_dict = {}\n",
    "final_conv_hist = {}\n",
    "\n",
    "# process convergence history\n",
    "for var in variables:\n",
    "    hist = {}\n",
    "    for i, (v, conv_i) in enumerate(reduce_hist):\n",
    "        if v == var:\n",
    "            hist['conv_order'] = i\n",
    "            hist['conv_i'] = conv_i\n",
    "            if i == 0:\n",
    "                hist['wtime'] = wtimes[conv_i]\n",
    "                hist['nit'] = (conv_i + 1) * 10\n",
    "            else:\n",
    "                hist['wtime'] = wtimes[conv_i] - wtimes[reduce_hist[i - 1][1]]\n",
    "                hist['nit'] = (conv_i + 1) * 10 - (reduce_hist[i - 1][1] + 1) * 10\n",
    "            break\n",
    "    final_conv_hist[var] = hist\n",
    "    final_w_dict[var] = res_hist[hist['conv_i']]['w_dict'][var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ed7d3",
   "metadata": {},
   "source": [
    "Next, compute the uncertainty of the MLE because we have disabled uncertainty computation during inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e4ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not read_results_from_file:\n",
    "    # compute uncertainty\n",
    "    G = Cylinder.compute_G_mini_batch([qx, qy], par_dict, {'drho': drho}, \n",
    "                                      batch_size=batch_size, device=device,\n",
    "                                      fixed_par_weights=None)\n",
    "    g_sys = SASGreensSystem(G, variables, batch_size=batch_size, device=device)\n",
    "    res_dict_uncert = g_sys.solve_inverse(I_true, I_true, auto_scaling=True, \n",
    "                                          maxiter=1, verbose=2, trust_options={'xtol': 0.},\n",
    "                                          w_dict_init=final_w_dict, xi_init=final_xi, b_init=final_b,\n",
    "                                          returns_intensity_sensitivity_uncertainty=True)\n",
    "    del G, g_sys\n",
    "\n",
    "    # save so we can skip this cell next time\n",
    "    with open(output_dir / 'res_dict_uncert.pkl', 'wb') as f:\n",
    "        pickle.dump(res_dict_uncert, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read uncertainty\n",
    "with open(output_dir / 'res_dict_uncert.pkl', 'rb') as f:\n",
    "    res_dict_uncert = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52122e4d",
   "metadata": {},
   "source": [
    "# Visualizing results\n",
    "\n",
    "Plot the parameter distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.rcParams.update({'legend.fontsize': 12})\n",
    "plt.rcParams.update({'axes.titlesize': 14})\n",
    "plt.rcParams.update({'lines.linewidth': 1.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, dpi=200, figsize=(8., 5))\n",
    "plt.subplots_adjust(hspace=.65, wspace=.25)\n",
    "axes = [ax[0, 0], ax[0, 1], ax[1, 0], ax[1, 1]]\n",
    "\n",
    "# handcraft variables in latex\n",
    "final_conv_hist['phi']['vars'] = 'l, r, \\\\theta, \\\\phi'\n",
    "final_conv_hist['r']['vars'] = 'l, r, \\\\theta'\n",
    "final_conv_hist['l']['vars'] = 'l, \\\\theta'\n",
    "final_conv_hist['theta']['vars'] = '\\\\theta'\n",
    "\n",
    "for i, axis in enumerate(axes):\n",
    "    var = variables[i]\n",
    "    if var in ['theta', 'phi']:\n",
    "        x = torch.rad2deg(par_dict[var])\n",
    "    else:\n",
    "        x = par_dict[var]\n",
    "    # true\n",
    "    ptrue = axis.plot(x, w_true_dict[var] * 100, '-o', lw=0, label='Truth', \n",
    "                      markersize=5, markerfacecolor='none')\n",
    "    # inverted\n",
    "    pmean = axis.plot(x, final_w_dict[var] * 100, label='Inverted')\n",
    "    psigma = axis.fill_between(x, \n",
    "                               (final_w_dict[var] - res_dict_uncert['std_w_dict'][var] * 3) * 100, \n",
    "                               (final_w_dict[var] + res_dict_uncert['std_w_dict'][var] * 3) * 100, \n",
    "                               alpha=.4, color='gray', zorder=-100)\n",
    "    # convergence\n",
    "    axis.set_title(r'\\textcircled{\\raisebox{-0.9pt}{%d}} %d iters on $\\{%s\\}$, wt=%d s' % (\n",
    "        final_conv_hist[var]['conv_order'] + 1, final_conv_hist[var]['nit'],\n",
    "        final_conv_hist[var]['vars'], final_conv_hist[var]['wtime']), fontsize=14)\n",
    "    # error\n",
    "    x = .05 if i != 2 else .55\n",
    "    axis.text(x, .85, r'$|\\Delta\\mathbf{w}|$=%.1E' % (torch.norm(\n",
    "        w_true_dict[var] - final_w_dict[var])), va='center', transform=axis.transAxes)\n",
    "\n",
    "# settings\n",
    "ax[0, 0].set_xlabel(r'Length, $l$ (\\AA)')\n",
    "ax[0, 1].set_xlabel(r'Radius, $r$ (\\AA)')\n",
    "ax[1, 0].set_xlabel(r'Axis-to-beam angle, $\\theta$ ($^\\circ$)')\n",
    "ax[1, 1].set_xlabel(r'Rotation about beam, $\\phi$ ($^\\circ$)')\n",
    "ax[0, 0].set_ylabel(r'$w(l)$ (\\%)')\n",
    "ax[0, 1].set_ylabel(r'$w(r)$ (\\%)')\n",
    "ax[1, 0].set_ylabel(r'$w(\\theta)$ (\\%)')\n",
    "ax[1, 1].set_ylabel(r'$w(\\phi)$ (\\%)')\n",
    "ax[0, 0].set_xlim(par_dict['l'].min(), par_dict['l'].max())\n",
    "ax[0, 1].set_xlim(par_dict['r'].min(), par_dict['r'].max())\n",
    "ax[1, 0].set_xlim(torch.rad2deg(par_dict['theta'].min()), torch.rad2deg(par_dict['theta'].max()))\n",
    "ax[1, 1].set_xlim(torch.rad2deg(par_dict['phi'].min()), torch.rad2deg(par_dict['phi'].max()))\n",
    "ax[0, 0].set_yticks([0, 3, 6, 9])\n",
    "ax[0, 1].set_yticks([0, 4, 8])\n",
    "ax[1, 0].set_yticks([0, 4, 8])\n",
    "ax[1, 1].set_yticks([0, 4, 8])\n",
    "\n",
    "ax[0, 1].set_ylim(-3, 8)\n",
    "ax[0, 1].legend(\n",
    "    [ptrue[0], (psigma, pmean[0])], ['Truth', 'Inverted'],\n",
    "    ncol=2, columnspacing=1, handlelength=1, prop={'size':12}, loc='lower center')\n",
    "\n",
    "if reproduce_paper_fig:\n",
    "    plt.savefig(paper_fig_dir / 'cylinder_w.pdf', \n",
    "                bbox_inches='tight', facecolor='w', pad_inches=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d96e3",
   "metadata": {},
   "source": [
    "Finally, plot the fitted intensity. Here we choose a different discretization of $q_x$ and $q_y$ for better visual effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3072e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not read_results_from_file:\n",
    "    # q vectors\n",
    "    # log scale on the sides\n",
    "    q_side = torch.logspace(-1.3, 0, 50)\n",
    "    # linear scale in the center\n",
    "    q_center = torch.linspace(-0.05, 0.05, 20)\n",
    "    qx_v = torch.cat((-torch.from_numpy(q_side.numpy()[::-1].copy()), q_center, q_side))\n",
    "    qy_v = qx_v.clone()\n",
    "\n",
    "    # compute the Green's tensor\n",
    "    G = Cylinder.compute_G_mini_batch([qx_v, qy_v], par_dict, {'drho': drho}, \n",
    "                                      batch_size=batch_size, device=device,\n",
    "                                      fixed_par_weights=None)\n",
    "\n",
    "    # define the G-based SAS system\n",
    "    g_sys = SASGreensSystem(G, ['l', 'r', 'theta', 'phi'], batch_size=batch_size, device=device)\n",
    "\n",
    "    # finally compute the ground truth of intensity\n",
    "    I_fit = g_sys.compute_intensity(final_w_dict, final_xi, final_b)\n",
    "\n",
    "    # free large memory of G\n",
    "    del G, g_sys\n",
    "\n",
    "    # save\n",
    "    with open(output_dir / 'I_fit.pkl', 'wb') as f:\n",
    "        pickle.dump(I_fit, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18acf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# read intenstiy\n",
    "with open(output_dir / 'I_fit.pkl', 'rb') as f:\n",
    "    I_fit = pickle.load(f)\n",
    "    \n",
    "fig = plt.figure(dpi=200, figsize=(5, 5))\n",
    "im = plt.imshow(I_fit.t(), \n",
    "                extent=(qx[0], qx[-1], qy[0], qy[-1]), aspect=1., cmap='turbo',\n",
    "                norm=colors.LogNorm(vmin=I_true.min(), vmax=I_true.max()))\n",
    "plt.title(r'Intensity image, $I(q_x, q_y)$ ($\\mathrm{cm}^{-1}$)', fontsize=18, pad=15)\n",
    "plt.axis('off')\n",
    "\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.new_vertical(size=\"7%\", pad=.3, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "cbar=fig.colorbar(im, cax=cax, orientation=\"horizontal\", )\n",
    "cbar.set_ticks([.001, .01, .1, 1, 10, 100])\n",
    "cbar.ax.tick_params(labelsize=17)\n",
    "\n",
    "if reproduce_paper_fig:\n",
    "    plt.savefig(paper_fig_dir / 'cylinder_I.pdf', bbox_inches='tight', facecolor='w', pad_inches=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29959d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WCT low-resolutoin:', res_dict_low['wct'])\n",
    "print('WCT high-resolutoin:',  wtimes[-1])\n",
    "print('WCT total:', res_dict_low['wct'] +  wtimes[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e1e8f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
